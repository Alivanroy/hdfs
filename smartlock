#!/bin/bash

# Script to download files from HDFS through Knox Gateway with concurrent execution support
# Usage: ./knox_hdfs_download.sh <base_path> <app_name> [date]

# Exit on any error
set -e

# Configuration
KNOX_HOST="web:9443"
KNOX_BASE_PATH="/gateway/cdp-proxy-api/webhdfs/v1"
OUTPUT_DIR="/opt/splunk/data"
LOG_DIR="/var/log/hdfs_downloads"
LOG_RETENTION_DAYS=7

# Credentials
KNOX_USER="your_username_here"
KNOX_PASSWORD="your_password_here"

# Generate unique process ID
PROCESS_ID="$$_$(date +%s%N)"

# Lock file directory
LOCK_DIR="/var/lock/hdfs_downloads"
mkdir -p "$LOCK_DIR"

# Function to validate date format
validate_date() {
    local date_str="$1"
    if [[ ! "$date_str" =~ ^[0-9]{8}$ ]]; then
        log_message "ERROR" "Invalid date format. Expected YYYYMMDD" "\"date\":\"${date_str}\""
        return 1
    fi
    return 0
}

# Function to acquire lock
acquire_lock() {
    local lock_file="$1"
    local wait_time=0
    local max_wait=300  # 5 minutes maximum wait time
    
    while ! mkdir "$lock_file" 2>/dev/null; do
        wait_time=$((wait_time + 1))
        if [ $wait_time -ge $max_wait ]; then
            echo "ERROR: Could not acquire lock after 5 minutes"
            return 1
        fi
        sleep 1
    done
    return 0
}

# Function to release lock
release_lock() {
    local lock_file="$1"
    rm -rf "$lock_file"
}

# Setup logging with process isolation
setup_logging() {
    local app="$1"
    local date_path="$2"
    
    mkdir -p "$LOG_DIR"
    
    # Define log file with date and process ID
    LOG_FILE="${LOG_DIR}/${app}_${date_path}_${PROCESS_ID}.log"
    
    touch "$LOG_FILE"
    
    # Rotate old logs (using lock to prevent concurrent rotation)
    local rotation_lock="${LOCK_DIR}/rotation.lock"
    if acquire_lock "$rotation_lock"; then
        find "$LOG_DIR" -name "${app}_*.log" -type f -mtime +${LOG_RETENTION_DAYS} -delete
        release_lock "$rotation_lock"
    fi
}

# Function to log messages with JSON format and process ID
log_message() {
    local level="$1"
    local message="$2"
    local additional_fields="$3"
    
    local timestamp=$(date -u '+%Y-%m-%dT%H:%M:%S.000Z')
    
    local log_entry="{\"timestamp\":\"${timestamp}\",\"level\":\"${level}\",\"app\":\"${APP_NAME}\",\"process_id\":\"${PROCESS_ID}\",\"message\":\"${message}\",\"date_path\":\"${DATE_PATH}\",\"base_path\":\"${BASE_PATH}\""
    
    if [ ! -z "$additional_fields" ]; then
        log_entry="${log_entry},${additional_fields}"
    fi
    
    log_entry="${log_entry}}"
    
    # Use flock for atomic writes to log file
    (
        flock -x 200
        echo "$log_entry" >> "$LOG_FILE"
    ) 200>"$LOG_FILE.lock"
    
    # Also output to stdout for Control-M
    echo "$log_entry"
}

# Function to safely write to consolidated log
write_to_consolidated_log() {
    local app="$1"
    local date_path="$2"
    local consolidated_log="${LOG_DIR}/${app}_${date_path}.log"
    local lock_file="${LOCK_DIR}/${app}_${date_path}.lock"
    
    if acquire_lock "$lock_file"; then
        cat "$LOG_FILE" >> "$consolidated_log"
        release_lock "$lock_file"
    fi
}

# Function to get file size from HDFS
get_file_size() {
    local file_path="$1"
    local size=$(curl -s -k -I -u "${KNOX_USER}:${KNOX_PASSWORD}" \
        "https://${KNOX_HOST}${KNOX_BASE_PATH}${file_path}?op=GETFILESTATUS" | \
        grep -i "Content-Length" | awk '{print $2}' | tr -d '\r')
    echo "${size:-0}"
}

# Function to format file size
format_size() {
    local size=$1
    if [ $size -ge 1073741824 ]; then
        echo "$(awk "BEGIN {printf \"%.2f\", $size/1073741824}")GB"
    elif [ $size -ge 1048576 ]; then
        echo "$(awk "BEGIN {printf \"%.2f\", $size/1048576}")MB"
    elif [ $size -ge 1024 ]; then
        echo "$(awk "BEGIN {printf \"%.2f\", $size/1024}")KB"
    else
        echo "${size}B"
    fi
}

# Function to safely move files to final location
move_to_final_location() {
    local temp_file="$1"
    local final_file="$2"
    local lock_file="${LOCK_DIR}/$(basename "$final_file").lock"
    
    if acquire_lock "$lock_file"; then
        mv "$temp_file" "$final_file"
        release_lock "$lock_file"
        return 0
    fi
    return 1
}

# Modified download_file function with temporary directory
download_file() {
    local file_path="$1"
    local filename=$(basename "$file_path")
    local temp_file="${TEMP_DIR}/${filename}"
    local final_file="${LOCAL_DIR}/${filename}"
    
    # Skip if file already exists in final location
    if [ -f "$final_file" ]; then
        log_message "INFO" "File already exists" "\"file\":\"${filename}\",\"status\":\"skipped\""
        return 0
    fi
    
    local file_size=$(get_file_size "$file_path")
    local formatted_size=$(format_size "$file_size")
    
    log_message "INFO" "Starting download" "\"file\":\"${filename}\",\"size\":\"${formatted_size}\""
    
    local start_time=$(date +%s)
    
    # Download to temporary location
    curl -s -k -L -o "$temp_file" \
        -u "${KNOX_USER}:${KNOX_PASSWORD}" \
        "https://${KNOX_HOST}${KNOX_BASE_PATH}${file_path}?op=OPEN"
    
    local end_time=$(date +%s)
    local duration=$((end_time - start_time))
    
    if [ $? -ne 0 ] || [ ! -s "$temp_file" ]; then
        log_message "ERROR" "Download failed" "\"file\":\"${filename}\",\"size\":\"${formatted_size}\""
        rm -f "$temp_file"
        return 1
    fi
    
    chmod 644 "$temp_file"
    
    # Move to final location
    if move_to_final_location "$temp_file" "$final_file"; then
        log_message "INFO" "Download completed" "\"file\":\"${filename}\",\"size\":\"${formatted_size}\",\"duration_seconds\":${duration},\"status\":\"success\""
        return 0
    else
        log_message "ERROR" "Failed to move file to final location" "\"file\":\"${filename}\""
        rm -f "$temp_file"
        return 1
    fi
}

# Cleanup function
cleanup() {
    # Remove temporary directory
    rm -rf "$TEMP_DIR"
    
    # Consolidate logs
    write_to_consolidated_log "$APP_NAME" "$DATE_PATH"
    rm -f "$LOG_FILE" "$LOG_FILE.lock"
}

# Register cleanup on script exit
trap cleanup EXIT

# Main execution
main() {
    log_message "INFO" "Starting HDFS download process"
    
    # List files in HDFS directory
    local files_list=$(curl -s -k -L \
        -u "${KNOX_USER}:${KNOX_PASSWORD}" \
        "https://${KNOX_HOST}${KNOX_BASE_PATH}${HDFS_PATH}?op=LISTSTATUS" | \
        grep -o '"pathSuffix":"[^"]*"' | cut -d'"' -f4)
    
    if [ -z "$files_list" ]; then
        log_message "ERROR" "No files found or error listing directory" "\"path\":\"${HDFS_PATH}\""
        exit 1
    fi
    
    # Download each file
    local success_count=0
    local total_files=0
    
    while IFS= read -r file; do
        if [ ! -z "$file" ]; then
            total_files=$((total_files + 1))
            if download_file "${HDFS_PATH}/${file}"; then
                success_count=$((success_count + 1))
            fi
        fi
    done <<< "$files_list"
    
    log_message "INFO" "Download process completed" "\"total_files\":${total_files},\"successful_downloads\":${success_count}"
    
    if [ $success_count -ne $total_files ]; then
        log_message "ERROR" "Some downloads failed" "\"failed_downloads\":$((total_files - success_count))"
        exit 1
    fi
}

# Input parameters
BASE_PATH="$1"
APP_NAME="$2"

# Handle date parameter
if [ -z "$3" ]; then
    DATE_PATH=$(date '+%Y%m%d')
else
    DATE_PATH="$3"
    if ! validate_date "$DATE_PATH"; then
        exit 1
    fi
fi

# Setup logging with process isolation
setup_logging "$APP_NAME" "$DATE_PATH"

# Construct full HDFS path
HDFS_PATH="${BASE_PATH}/${DATE_PATH}"

# Setup local paths with process isolation
LOCAL_DIR="${OUTPUT_DIR}/${APP_NAME}/${DATE_PATH}"
TEMP_DIR="${OUTPUT_DIR}/${APP_NAME}/temp/${PROCESS_ID}"
mkdir -p "$LOCAL_DIR"
mkdir -p "$TEMP_DIR"

# Execute main function
main
